{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (7.1.0)\n",
      "Requirement already satisfied: prawcore<2.0,>=1.3.0 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from praw) (1.5.0)\n",
      "Requirement already satisfied: update-checker>=0.17 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from praw) (0.57.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from prawcore<2.0,>=1.3.0->praw) (2.25.0)\n",
      "Requirement already satisfied: six in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (1.25.10)\n",
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\hithe\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install PRAW, TextBlob modules if not already present\n",
    "!pip install praw\n",
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hithe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download the stopwords for english language\n",
    "\n",
    "nltk.download('punkt')\n",
    "stopwords.words('english')\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PRAW reddit client to extract comments.\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=\"Comment Extraction for Simple Sentiment Analysis\",\n",
    "    client_id=\"CLIENT_ID\",\n",
    "    client_secret=\"CLIENT_SECRET\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the extracted comments\n",
    "\n",
    "def process(sent):\n",
    "    tokens = nltk.word_tokenize(sent) #tokenize the comments\n",
    "    words = [w.lower() for w in tokens if w not in stopwords] # Remove any stopwords from the comments\n",
    "    wnl = nltk.WordNetLemmatizer() \n",
    "    words=[wnl.lemmatize(w) for w in words] # USe WordNet Lemmatizer to lemmatize the tokens\n",
    "    \n",
    "    tb=TextBlob(' '.join(words)) # Create a textblob object to calculate the polarity of the words.\n",
    "      \n",
    "    print(\"Result: \")\n",
    "    print(\"#############################################################################################################################\")\n",
    "    print(\"Sentiment of the word on a scale between -1.0 - 1.0 = \",tb.sentiment.polarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw.models import MoreComments\n",
    "\n",
    "# Driver Function to analyze the sentiment in the reddit comments\n",
    "\n",
    "def RCA(ID):\n",
    "    \n",
    "    comm=[]\n",
    "    \n",
    "    subreddit=reddit.subreddit(\"all\")\n",
    "    \n",
    "    for submission in subreddit.search(ID, sort='comments',limit=5):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            comm.append(top_level_comment.body)\n",
    "            \n",
    "    process(' '.join(comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a keyword to find it's polarity in the 'all' subreddit: Avengers\n",
      "Result: \n",
      "#############################################################################################################################\n",
      "Sentiment of the word on a scale between -1.0 - 1.0 =  0.12776965783495858\n"
     ]
    }
   ],
   "source": [
    "ID=input(\"Enter a keyword to find it's polarity in the 'all' subreddit: \")\n",
    "RCA(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
